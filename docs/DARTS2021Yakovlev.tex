\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\bibliographystyle{jmlda-rus.bst}
\newcommand{\hdir}{.}



\begin{document}

\title
    [Дифференцируемый алгоритм поиска архитектуры модели с контролем её сложности] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Дифференцируемый алгоритм поиска архитектуры модели с контролем её сложности}
\author
    [К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев} % основной список авторов, выводимый в оглавление
    [К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    { iakovlev.kd@phystech.edu; grebenkova.os@phystech.edu; bakhteev@phystech.edu}

\abstract
    {В работе исследуется задача построения модели глубокого обучения. Предлагается метод поиска архитектуры модели, позволяющий контролировать её сложность с небольшими вычислительными затратами. Под сложностью модели понимается минимальная длина описания,
минимальное количество информации, которое требуется для передачи информации о модели и о выборке. В основе метода лежит дифференцируемый алгоритм поиска архитектуры модели (DARTS). Контроль сложности параметров производится гиперсетью. Предлагается использовать гиперсеть в качестве функции релаксации. Предложенный метод позволяет контролировать сложность модели в процессе поиска архитектуры.
	
\bigskip
\noindent
\textbf{Ключевые слова}: \emph {}
}



%данные поля заполняются редакцией журнала
%\doi{10.21469/22233792}
\receivedRus{25.02.2021}
\receivedEng{February 25, 2021}


\maketitle
\linenumbers

\section{Введение}

В данной работе рассматривается оптимизации модели глубокого обучения с контролем её сложности. В качестве базового алгоритма используется DARTS. В работе \cite{journals/corr/abs-1806-09055} рассматривается проблема поиска архитектуры путем непрерывного представления архитектуры модели. Градиентные методы оптимизации позволяют использовать меньше вычислительных ресурсов. Данный алгоритм универсален для работы как со сверточными, так и с рекуррентными архитектурами нейронных сетей.

В работе \cite{journals/corr/abs-2002-05283} устойчивость алгоритма DARTS была поставлена под сомнение. Одним из источников неустойчивости является этап получения фактической дискретной архитектуры из архитектуры непрерывной смеси. На этом этапе часто наблюдается снижение качества модели. В данной работе веса модели формируются как минимизатор случайно сглаженной функции, определяемой как ожидаемая потеря в окрестности текущей архитектуры. В работе \cite{journals/corr/abs-1911-12126} предлагается использовать 0-1 функцию потерь для уменьшения расхождения между дискретной архитектурой и архитектурой непрерывной смеси.

Предлагаются альтернативные подходы к решению задачи. В работе \cite{journals/corr/abs-2006-10355} формулируется задача обучения распределению с ограничениями. Предложенный метод может быть эффективно оптимизирован и обладает теоретическими преимуществами для повышения способности к обобщению.

В работе \cite{journals/corr/abs-1912-12814} строится алгоритм поиска нейронной архитектуры с ограниченным ресурсом (RC-DARTS). К базовому алгоритму DARTS добавляются ресурсные ограничения. Для решения задачи условной оптимизации вводится алгоритм итерационной проекции.

Работе \cite{journals/corr/HaDL16} исследует гиперсети. Подход заключается в использовании небольшой сети для генерации весов более крупной сети. Рассматривались два варианта использования гиперсетей: статические гиперсети для генерации весов для сверточной сети и динамические гиперсети для генерации весов рекуррентной сети.

Вычислительный эксперимент будет проводиться на выборках \cite{lecun-mnisthandwrittendigit-2010} и \cite{cif}.

\newpage
\section{Постановка задачи}

%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/
\bibliography{Version1.bib}
\end{document}

