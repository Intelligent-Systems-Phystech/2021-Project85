\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\bibliographystyle{jmlda-rus.bst}
\newcommand{\hdir}{.}

\usepackage{graphicx}
%\graphicspath{ {../fig/} {.}}
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{multirow}



\begin{document}

\title
    [Differentiable algorithm for searching the model architecture with control of its complexity] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Differentiable algorithm for searching the model architecture with control of its complexity}
\author
    [K.\,D.~Yakovlev, O.\,S.~Grebenkova, O.\,Y.~Bakhteev] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {K.\,D.~Yakovlev, O.\,S.~Grebenkova, O.\,Y.~Bakhteev} % основной список авторов, выводимый в оглавление
    [K.\,D.~Yakovlev, O.\,S.~Grebenkova, O.\,Y.~Bakhteev] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    { iakovlev.kd@phystech.edu; grebenkova.os@phystech.edu; bakhteev@phystech.edu}

\abstract
{The paper investigates the problem of deep learning model optimization. The authors propose a method for finding the architecture of a model that allows you to control its complexity with a small computational cost. The complexity of the model refers to the minimum length of the description,
the minimum amount of information required to transmit information about the model and the dataset. The method is based on a differentiable architecture search algorithm (DARTS). It is proposed to use the hypernet as a relaxation function. A hypernet is a model that generates the parameters of an optimal model. The proposed method allows you to control the complexity of the model in the process of searching for an architecture. To assess the quality of the proposed algorithm, experiments are conducted on a sample of MNIST.
	
\bigskip
\noindent
\textbf{Keywords}: \emph {differentiable architecture search; deep learning; hypernetwork; neural networks; model complexity control}
}



%данные поля заполняются редакцией журнала
%\doi{10.21469/22233792}
\receivedRus{25.02.2021}
\receivedEng{February 25, 2021}


\maketitle
\linenumbers

\title
    [Дифференцируемый алгоритм поиска архитектуры модели с контролем её сложности] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Дифференцируемый алгоритм поиска архитектуры модели с контролем её сложности}
\author
    [К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев} % основной список авторов, выводимый в оглавление
    [К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    { iakovlev.kd@phystech.edu; grebenkova.os@phystech.edu; bakhteev@phystech.edu}

\abstract
    {В работе исследуется задача построения модели глубокого обучения. Предлагается метод поиска архитектуры модели, позволяющий контролировать её сложность с небольшими вычислительными затратами. Под сложностью модели понимается минимальная длина описания,
минимальное количество информации, которое требуется для передачи информации о модели и выборке. В основе метода лежит дифференцируемый алгоритм поиска архитектуры модели (DARTS). Предлагается использовать гиперсеть в качестве функции релаксации.  Под гиперсетью понимается модель, генерирующуя параметры оптимальной модели. Предложенный метод позволяет контролировать сложность модели в процессе поиска архитектуры. Для оценки качества предлагаемого алгоритма проводятся эксперименты на выборке MNIST.

	
\bigskip
\noindent
\textbf{Ключевые слова}: \emph {дифференцируемый алгоритм поиска архитектуры; глубокое обучение; гиперсети; нейронные сети; контроль сложности модели}
}



%данные поля заполняются редакцией журнала
%\doi{10.21469/22233792}
\receivedRus{25.02.2021}
\receivedEng{February 25, 2021}


\maketitle
\linenumbers

\section{Введение}


В данной работе рассматривается задача поиска архитектуры модели глубокого обучения с контролем её сложности. Под моделью понимается суперпозиция функций, решающая задачу классификации или регрессии \cite{journals/aarc/BakhteevS18}. Под поиском архитектуры модели понимается поиск оптимальных структурных параметров. Под релаксацией понимается перевод множества допустимых структурных параметров из дискретного в непрерывное. В качестве базового алгоритма используется дифференцируемый алгоритм поиска архитектуры DARTS \cite{journals/corr/abs-1806-09055}. Он решает задачу поиска архитектуры модели путем перевода пространства поиска структурных параметров из дискретного в непрерывное представление. Предлагается использовать градиентные методы оптимизации. Они используют меньше вычислительных ресурсов, чем методы, работающие на дискретном множестве структурных параметров. Данный алгоритм работает как со сверточными, так и с рекуррентными нейронными сетями.

В \cite{journals/corr/abs-2002-05283} указано, что DARTS нестабилен. Это связано с тем, что параметры архитектуры модели сходятся в узкий регион. Поэтому небольшие возмущения архитектуры ведут к значительному понижению качества.

В \cite{journals/corr/abs-1911-12126} замечено, что функция $\text{softmax}$ обладает существенным недостатком. Некоторые компоненты вектора параметров архитектуры увеличиваются быстрее, чем остальные. Это приводит к тому, что архитектура переупрощается и ,как следствие, ухудшается качество\cite{journals/corr/abs-1911-12126}. В связи с этим предлагается использовать сигмоидную  функцию релаксации и отказаться от нормировки. Таким образом, все параметры архитектуры изменяются с одинаковой скоростью. В данной работе предлагается в качестве функции релаксации использовать гиперсеть \cite{journals/corr/HaDL16}. Подход заключается в использовании небольшой сети для генерации параметров архитектуры искомой сети. Также гиперсеть используется для контроля сложности.

 Предлагаются альтернативные DARTS подходы к решению задачи поиска архитектуры модели. В работе \cite{journals/corr/abs-2006-10355} формулируется задача обучения распределению. Параметры архитектуры подчинены распределению Дирихле, так как они определены на вероятностном симплексе. Таким образом, задача поиска архитектуры сводится к поиску параметров распределения Дирихле.

В работе \cite{journals/corr/abs-1912-12814} строится метод поиска нейронной архитектуры с ограниченным ресурсом (RC-DARTS). К базовому алгоритму DARTS добавляются ограничения, такие как число параметров модели. Для решения задачи условной оптимизации вводится алгоритм итерационной проекции, заключающийся в том, что через определенное число итераций градиентного спуска происходит проецирование на множество, задаваемое ограничениями.


Вычислительный эксперимент проводится на выборке MNIST\cite{lecun-mnisthandwrittendigit-2010}.

\section{Постановка задачи}

\subsection{Дифференцируемый алгоритм поиска архитектуры ячейки}

Поставим задачу поиска архитектуры ячейки.
 Ячейка представляет собой $N$ занумерованных узлов, пердставленных в виде ориентированного ациклического графа. Каждому ребру $(i, j)$ поставлено в соответствие  отображение (операция) $o^{(i, j)} \in \mathcal{O}$, где $\mathcal{O}$ -- семейство отображений (множество операций). Значения в каждом из промежуточных узлов $x^{(j)}$ определяются через значения в узлах с меньшим номером:
 
 \begin{equation}
 x^{(j)} = \sum_{i < j}o^{(i, j)}(x^{(i)}).
 \end{equation}
 
 \begin{figure}[H]
 \centering
 \includegraphics[width=0.8\textwidth]{reduction.eps}
 \caption{Пример найденной архитектуры}
 \end{figure}
 
 Таким образом, задача поиска архитектуры заключается в выборе отображения между узлами ячейки. Для того, чтобы свести задачу дискретной оптимизации к задаче непрерывной оптимизации, введем смешанную операцию для каждого ребра $(i, j)$:

\begin{equation}
\hat{o}^{(i, j)}(x) = \sum_{o\in \mathcal{O}} \frac{\exp(\alpha_o^{(i, j)})}{\sum_{o'\in\mathcal{O}} \exp(\alpha_{o'}^{(i, j)})}o(x),
\end{equation}
где $\alpha_o^{(i, j)}$ обозначает соответствующий вес операции $o$ на ребре $(i, j)$. Таким образом, каждому ребру $(i, j)$ ставится в соответствие вектор $\alpha^{(i, j)}$ размерности $|\mathcal{O}|$. Пусть $\boldsymbol\alpha = [\alpha^{(i, j)}]$. Сформулируем двухуровневую задачу оптимизации:

\begin{equation}
\label{optim}
\begin{aligned}
\min_{\alpha}\mathcal{L}_\text{val}(\mathbf{w}^*, \alpha),\\
 \mathrm{s.t.}\quad \mathbf{w}^* = \arg\min_{\mathbf{w}}\mathcal{L}_\text{train}(\mathbf{w}, \alpha)
 \end{aligned}
\end{equation}
Здесь $\mathcal{L}_\text{val}$ и $\mathcal{L}_\text{train}$ функции потерь модели на валидации и на обучении соответственно.

\subsection{Линейная гиперсеть}

Пусть $\Lambda$ --  множество параметров, контролирующие сложность модели. Гиперсеть -- это отображение:

\begin{equation}
	\mathbf{G} : \Lambda \times \mathbb{U} \rightarrow \mathbb{A},
\end{equation}
где $\mathbb{A}$ -- пространство параметров архитектуры $\{\alpha^{(i, j)}_o\}$, а $\mathbb{U}$ -- множество параметров гиперсети.
В данной работе для получения весов операций используется линейная гиперсеть:
 
 \begin{equation}\label{hypernet}
 \mathbf{G}_{\text{linear}}(\lambda) = \lambda \mathbf{b}_1 + \mathbf{b}_2, \quad [\mathbf{b}_1, \mathbf{b}_2]^\top \in \mathbb{U}
 \end{equation}
 где $\mathbf{b}_1, ~\mathbf{b}_2$  настраиваются согласно оптимизационной задаче \eqref{optim}.

\section{Описание алгоритма}
 В качестве базового алгоритма используется алгоритм, описанный в работе \cite{journals/corr/abs-1806-09055}. Идея состоит в приближении истинного градиента:
$$\nabla_\alpha\mathcal{L}_\text{val}\bigl(\mathbf{w}^*(\alpha), \alpha\bigr) \approx \nabla_\alpha\mathcal{L}_\text{val} \bigl( \mathbf{w} - \xi\nabla_\mathbf{w}\mathcal{L}_\text{train}(\mathbf{w}, \alpha), \alpha \bigr)$$
Пусть $\mathbf{w}'(\alpha) = \mathbf{w} - \xi\nabla_\mathbf{w}\mathcal{L}_\text{train}(\mathbf{w}, \alpha)$. Тогда получаем:
\begin{equation}
\label{darts:grad}
\nabla_\alpha\mathcal{L}_\text{val}\bigl(\mathbf{w}'(\alpha), \alpha\bigr) = \nabla_\alpha\mathcal{L}_\text{val}(\mathbf{w}, \alpha)\big|_{\mathbf{w} = \mathbf{w}'(\alpha)} - \xi\nabla^2_{\alpha, \mathbf{w}}\mathcal{L}_\text{train}(\mathbf{w}, \alpha)\nabla_\mathbf{w'}\mathcal{L}_\text{val}\bigl(\mathbf{w}'(\alpha), \alpha\bigr)
\end{equation}
Обозначим $\mathbf{w}^{\pm} = \mathbf{w}\pm\epsilon\nabla_\mathbf{w'}\mathcal{L}_\text{val}\bigl(\mathbf{w}'(\alpha), \alpha\bigr)$, где $\epsilon = 0.01\bigl(\|\nabla_\mathbf{w'}\mathcal{L}_\text{val}\bigl(\mathbf{w}'(\alpha), \alpha\bigr)\|_2\bigr)^{-1}$.
Тогда
\begin{equation}
\label{darts:hess_prod}
\nabla^2_{\alpha, \mathbf{w}}\mathcal{L}_\text{train}(\mathbf{w}, \alpha)\nabla_\mathbf{w'}\mathcal{L}_\text{val}\bigl(\mathbf{w}'(\alpha), \alpha\bigr) \approx \frac{\nabla_\alpha\mathcal{L}_\text{train}(\mathbf{w}^+, \alpha) - \nabla_\alpha\mathcal{L}_\text{train}(\mathbf{w}^-, \alpha)}{2\epsilon}
\end{equation}
Следовательно, градиент \eqref{darts:grad} вычисляется за $O(\dim\mathbf{w} + \dim\alpha)$. Заметим, что без оценки \eqref{darts:hess_prod} приходилось бы выполнять $O(\dim\mathbf{w}\dim\alpha)$ операций. Приведем псевдокод алгоритма:

 \begin{algorithm}[H]
\begin{algorithmic}[1]
\caption{DARTS -- Differentiable Architecture Search}
\label{alg:darts}
\STATE Для каждого узла создадим смешанную операцию $\hat{o}^{(i, j)}$, параметризованную $\alpha^{(i, j)}$
\WHILE{алгоритм не сошелся} 
\STATE  обновить $\alpha$, сделав градиентный шаг вдоль $\nabla_\alpha \mathcal{L}_\text{val}\bigl(\mathbf{w} - \xi\nabla_{\mathbf{w}}\mathcal{L}_\text{train}(\mathbf{w}, \alpha), \alpha\bigr)$
\STATE обновить веса $\mathbf{w}$, сделав градиентный шаг вдоль $\nabla_\mathbf{w}\mathcal{L}_\text{train}(\mathbf{w}, \alpha)$
\ENDWHILE
\STATE получить окончательную архитектуру из полученного $\alpha$
\end{algorithmic}
\end{algorithm}
Дискретная архитектура получается следующим образом:

$$o^{(i, j)} =\arg\max_{o\in\mathcal{O}}\alpha_o^{(i, j)}.$$
В предлагаемом алгоритме смешанная операция определяется как:
 
 $$\hat{o}^{(i, j)}(x) = \sum_{o\in \mathcal{O}}\alpha^{(i, j)}_oo(x).$$
Вектор параметров архитектуры модели определяется линейной гиперсетью:

$$\alpha = \lambda\mathbf{b}_1 + \mathbf{b}_2.$$
Одним из свойств предлагаемого решения является то, что изменение сложности модели происходит заменой параметра $\lambda$ гиперсети без дополнительного обучения.


 
 \section{Вычислительный эксперимент}
 
 \subsection{Базовый эксперимент}
 Целью базового эксперимента является получение зависимости качества работы алгоритма DARTS с регуляризатором от количества эпох при разных значениях параметра регуляризации $\lambda$.
 
 Предлагается использовать алгоритм DARTS с регуляризатором $\lambda\sum_{i=1}^{\dim\alpha}\mathbf{v}_i|\alpha_i|$ в качестве второго слагаемого в функции потерь, где $\mathbf{v}$ -- вектор весов операций. Чем сложнее операция, тем больше соответствующий элемент $\mathbf{v}$. Вычислительный эксперимент проводится на выборке MNIST\cite{lecun-mnisthandwrittendigit-2010}, которая представляет собой набор рукописных цифр. В качестве $\mathcal{L}_\text{train}(\mathbf{w}, \alpha), ~\mathcal{L}_\text{val}(\mathbf{w}, \alpha)$ используется кросс-энтропия.
 
Эксперимент запускался пять раз при значениях $\lambda \in \{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\}$. На рисунке \ref{fig:basic_exp} представлен график зависимости точности (Precision) модели от числа эпох.

\begin{equation}
\mathrm{Precision} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}
\end{equation}

График показывает, что при большом значении параметра регуляризации $\lambda$ качество сильно падает. Это связано с тем, что в этом случае не выгодно брать операции с большим весом. Таким образом, модель становится переупрощенной.

 Также из графика видно, что при $\lambda \in \{10^{-4}, 10^{-3}\}$ качество модели значительно возрастает, что связано с тем, что в архитектуре появляются операции, имеющие большой вес.

\begin{figure}[H]
\centering
  \includegraphics[width=0.8\textwidth]{new_basic_exp.eps}
  \caption{Зависимость точности модели от числа прошедших эпох для разынх параметров регуляризации}
  \label{fig:basic_exp}
\end{figure}

\subsection{Основной эксперимент}

Целью основного эксперимента является получение зависимости качества работы предложенного метода от параметра гиперсети $\lambda \in \{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\}$. Эксперимент проводится на выборке MNIST \cite{lecun-mnisthandwrittendigit-2010}. В качестве $\mathcal{L}_\text{train}(\mathbf{w}, \alpha), ~\mathcal{L}_\text{val}(\mathbf{w}, \alpha)$ используется кросс-энтропия. Рассматривался поиск архитектуры сверточной нейронной сети с одним слоем и четырьмя узлами. Контроль сложности производился линейной гиперсетью \eqref{hypernet}. Обучение проводилось на протяжении 100 эпох. В процессе обучения параметр $\lambda$ выбирался равномерно из отрезка $[10^{-5}, 10^{-1}]$. Как видно из графика \ref{fig:main_exp}, для $\lambda = 10^{-1}$ качество модели заметно хуже, чем для других значениях $\lambda$. Это связано с тем, что штраф за сложность модели становится большим, поэтому происходит переупрощение модели. Также для каждой эпохи и для каждого $\lambda \in \{10^{-4}, 10^{-3}, 10^{-2}\}$ качество модели практически не меняется. Это означает, что качество модели заметно не ухудшается при уменьшении сложности.

\begin{figure}[H]
\centering
  \includegraphics[width=0.8\textwidth]{main_100_exp.eps}
  \caption{Зависимость качества модели от числа прошедших эпох для разных параметров $\lambda$ гиперсети.}
  \label{fig:main_exp}
\end{figure}

\subsection{Анализ ошибки}

\begin{table}[H]
\centering
	\begin{tabular}{ |c|c|c|c|c| }
	\hline
	 \multirow{2}{*}{Модель} & \multicolumn{4}{c|}{Precision, \%} \\ \cline{2-5}
	 		& эпоха 30 & эпоха 50 & эпоха 70 & эпоха 100\\
	 \hline
 	Hypernet, $\lambda = 10^{-1}$ &96,3500 & 95,4800 & 94,1200 & 90,6333  \\ 
 	Hypernet, $\lambda = 10^{-2}$ &95,8900 & 96,7167& 91,0633 & 95,3133 \\ 
 	Hypernet, $\lambda = 10^{-3}$ &95,9133 & 96,6200 & 90,6400 & 95,6400 \\ 
 	Hypernet, $\lambda = 10^{-4}$ &95,9733 & 96,5367 & 90,4067 & 95,7533\\
 	\hline
 	DARTS, $\lambda = 10^{-1}$ &86,6000 &87,3967 & 89,3433 & 89,5067 \\
 	DARTS, $\lambda = 10^{-2}$ & 84,1333 & 90,6333 &91,9100 & 92,7333\\
 	DARTS, $\lambda = 10^{-3}$ &97.6533 & 97.5800 & 98.0833 & 97,9467 \\
 	DARTS, $\lambda = 10^{-4}$ & \textbf{98,5800} &  \textbf{98,8867}&\textbf{98,9467}&\textbf{99,2400} \\
 	\hline
\end{tabular}
\caption{\label{tab:exp} Результаты базового и основного экспериментов. Приведены значения качества моделей на валидации.}
\end{table}


Из таблицы \ref{tab:exp} видно, что для модели Hypernet качество на валидации уменьшается с ростом номера эпохи. Это связано с тем, что происходит выбор более простой архитектуры, а именно, становится больше пропусков соединений. Данное свойство DARTS исследовалось в работе \cite{journals/corr/abs-1911-12126}.

\begin{figure}[H]
\centering
  \includegraphics[width=\textwidth]{learn_curve.eps}
  \caption{Зависимость качества на обучающей и тестовой выборках от числа прошедших эпох для разных значений параметров $\lambda$ регуляризатора в базовом эксперименте.}
  \label{fig:learn_curve}
\end{figure}
 Из графика \ref{fig:learn_curve} видно, что уже после 20 эпохи качество на обучающей и на тестовой выборках пререстаёт существенно меняться. Кроме того, данный график показывает отсутствие переобучения для всех $\lambda$.

\section{Заключение}

В данной работе рассматривалась задача поиска архитектуры модели с контролем сложности. Предложен метод, позволяющий контролировать сложность модели в процессе поиска архитектуры. Метод обладает тем свойством, что изменение сложности итоговой модели происходит заменой параметра $\lambda$ гиперсети без дополнительного обучения. Также результаты показывают, что данный метод сопоставим по качеству на валидационной выборке с DARTS.

В дальнейшем планируется провести прунинг сети и проанализировать качество получившейся модели, а также количество используемых ресурсов.



%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/
\newpage
\bibliography{Version1.bib}
 \nocite{*}
\end{document}

