\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\bibliographystyle{jmlda-rus.bst}
\newcommand{\hdir}{.}
%\usepackage[ruled,vlined]{algorithm2e}


\begin{document}

\title
    [Дифференцируемый алгоритм поиска архитектуры модели с контролем её сложности] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Дифференцируемый алгоритм поиска архитектуры модели с контролем её сложности}
\author
    [К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев} % основной список авторов, выводимый в оглавление
    [К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    { iakovlev.kd@phystech.edu; grebenkova.os@phystech.edu; bakhteev@phystech.edu}

\abstract
    {В работе исследуется задача построения модели глубокого обучения. Предлагается метод поиска архитектуры модели, позволяющий контролировать её сложность с небольшими вычислительными затратами. Под сложностью модели понимается минимальная длина описания,
минимальное количество информации, которое требуется для передачи информации о модели и о выборке. В основе метода лежит дифференцируемый алгоритм поиска архитектуры модели (DARTS). Контроль сложности параметров производится гиперсетью. Под гиперсетью понимается модель, генерирующуя параметры другой модели. Предлагается использовать гиперсеть в качестве функции релаксации. Предложенный метод позволяет контролировать сложность модели в процессе поиска архитектуры. Для оценки качества предлагаемого алгоритма проводятся эксаерименты на выборках MNIST и CIFAR-10.
	
\bigskip
\noindent
\textbf{Ключевые слова}: \emph {}
}



%данные поля заполняются редакцией журнала
%\doi{10.21469/22233792}
\receivedRus{25.02.2021}
\receivedEng{February 25, 2021}


\maketitle
\linenumbers

\section{Введение}

В последнее время растет интерес к разработке алгоритмических решений для автоматизации процесса проектирования архитектуры. Лучшие существующие алгоритмы поиска архитектуры требуют больших вычислительных затрат, несмотря на их высокую производительность.

В данной работе рассматривается задача поиска архитектуры модели глубокого обучения с контролем её сложности. В качестве базового алгоритма используется дифференцируемый алгоритм поиска архитектуры (DARTS) \cite{journals/corr/abs-1806-09055}. Данный метод решает задачу поиска архитектуры модели путем перевода пространства поиска из дискретного в непрерывное представление. Градиентные методы оптимизации позволяют использовать меньше вычислительных ресурсов. Данный алгоритм универсален для работы как со сверточными, так и с рекуррентными нейронными сетями.

В работе \cite{journals/corr/abs-2002-05283} стабильность алгоритма DARTS была поставлена под сомнение. Одним из источников нестабильности является этап получения фактической дискретной архитектуры из архитектуры непрерывной смеси. На этом этапе часто наблюдается снижение качества модели. В данной работе веса модели формируются как минимизатор случайно сглаженной функции, определяемой как ожидаемая потеря в окрестности текущей архитектуры. В работе \cite{journals/corr/abs-1911-12126} предлагается использовать 0-1 функцию потерь для уменьшения расхождения между дискретной архитектурой и архитектурой непрерывной смеси.

Предлагаются альтернативные подходы к решению задачи поиска архитектуры модели. В работе \cite{journals/corr/abs-2006-10355} формулируется задача обучения распределению с ограничениями. Предложенный метод может быть эффективно оптимизирован и обладает теоретическими преимуществами для повышения способности к обобщению.

В работе \cite{journals/corr/abs-1912-12814} строится алгоритм поиска нейронной архитектуры с ограниченным ресурсом (RC-DARTS). К базовому алгоритму DARTS добавляются ресурсные ограничения, такие как размер модели и вычислительная сложность. Для решения задачи условной оптимизации вводится алгоритм итерационной проекции.

Работе \cite{journals/corr/HaDL16} исследует гиперсети. Подход заключается в использовании небольшой сети для генерации весов более крупной сети. Рассматривались два варианта использования гиперсетей: статические гиперсети для генерации весов для сверточной сети и динамические гиперсети для генерации весов рекуррентной сети.

Вычислительный эксперимент будет проводиться на выборках MNIST\cite{lecun-mnisthandwrittendigit-2010} и CIFAR-10\cite{cif}.

\section{Постановка задачи}

\subsection{Дифференцируемый алгоритм поиска архитектуры}

Пусть $\mathcal{O}$ -- множество операций. Введем смешанную операцию для каждого ребра $(i, j)$:

\begin{equation}
\hat{o}^{(i, j)}(x) = \sum_{o\in \mathcal{O}} \frac{\exp(\alpha_o^{(i, j)})}{\sum_{o'\in\mathcal{O}} \exp(\alpha_{o'}^{(i, j)})}o(x),
\end{equation}
где $\alpha_o^{(i, j)}$ обозначает соответствующий вес операции $o$ на ребре $(i, j)$. Таким образом, каждому ребру $(i, j)$ ставится в соответствие вертор $\alpha^{(i, j)}$ размерности $|\mathcal{O}|$. Пусть $\alpha = [\alpha^{(i, j)}]$. Сформулируем двухуровневую задачу оптимизации:

\begin{equation}
\begin{aligned}
\min_{\alpha}\mathcal{L}_{val}(\mathbf{w}^*(\alpha), \alpha),\\
 \mathrm{s.t.}\quad \mathbf{w}^* = \arg\min_{\mathbf{w}}\mathcal{L}_{train}(\mathbf{w}, \alpha)
 \end{aligned}
\end{equation}
Здесь $\mathcal{L}_{val}$ и $\mathcal{L}_{train}$ функции потерь модели на валидации и на обучении соответственно.


\begin{algorithm}[H]
\begin{algorithmic}[1]
\caption{DARTS -- Differentiable Architecture Search}
\STATE Для каждого узла создадим смешанную операцию $\hat{o}^{(i, j)}$, параметризованную $\alpha^{(i, j)}$
\WHILE{алгоритм не сошелся} 
\STATE  обновим $\alpha$, сделав градиентный шаг вдоль $\nabla_\alpha \mathcal{L}_{val}(\mathbf{w} - \xi\nabla_{\mathbf{w}}\mathcal{L}_{train}(\mathbf{w}, \alpha), \alpha)$
\STATE обновим веса $\mathbf{w}$, делая градиентный шаг вдоль $\nabla_\mathbf{w}\mathcal{L}_{train}(\mathbf{w}, \alpha)$
\ENDWHILE
\STATE получить окончательную архитектуру из полученного в результате алгоритма $\alpha$
\end{algorithmic}
\end{algorithm}
\subsection{Линейная гиперсеть}

Пусть $\Lambda$ --  множество параметров, контролирующие сложность модели. Под гиперсетью мы будем понимать следующее отображение:

\begin{equation}
	\mathbf{G} : \Lambda \times \mathbb{U} \rightarrow \mathbb{W},
\end{equation}

где $\mathbb{W}$ -- множество параметров модели, а $\mathbb{U}$ -- множество параметров гиперсети.

 В данной работе рассматривается линейная гиперсеть:
 
 \begin{equation}
 \mathbf{G}_{linear}(\lambda) = \lambda \mathbf{b}_1 + \mathbf{b}_2,
 \end{equation}
 где $\mathbf{b}_1, \mathbf{b}_2$ -- не зависящие от $\lambda$ константы.

\newpage
%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/
\bibliography{Version1.bib}
\end{document}

