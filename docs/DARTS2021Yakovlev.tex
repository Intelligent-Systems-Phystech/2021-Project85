\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\bibliographystyle{jmlda-rus.bst}
\newcommand{\hdir}{.}
%\usepackage[ruled,vlined]{algorithm2e}


\begin{document}

\title
    [Дифференцируемый алгоритм поиска архитектуры модели с контролем её сложности] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Дифференцируемый алгоритм поиска архитектуры модели с контролем её сложности}
\author
    [К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев} % основной список авторов, выводимый в оглавление
    [К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    { iakovlev.kd@phystech.edu; grebenkova.os@phystech.edu; bakhteev@phystech.edu}

\abstract
    {В работе исследуется задача построения модели глубокого обучения. Предлагается метод поиска архитектуры модели, позволяющий контролировать её сложность с небольшими вычислительными затратами. Под сложностью модели понимается минимальная длина описания,
минимальное количество информации, которое требуется для передачи информации о модели и выборке. В основе метода лежит дифференцируемый алгоритм поиска архитектуры модели (DARTS). Предлагается использовать гиперсеть в качестве функции релаксации.  Под гиперсетью понимается модель, генерирующуя параметры другой модели. Предложенный метод позволяет контролировать сложность модели в процессе поиска архитектуры. Для оценки качества предлагаемого алгоритма проводятся эксперименты на выборках MNIST и CIFAR-10.
	
\bigskip
\noindent
\textbf{Ключевые слова}: \emph {дифференцируемый алгоритм поиска архитектуры; глубокое обучение; гиперсети; нейронные сети; контроль сложности модели}
}



%данные поля заполняются редакцией журнала
%\doi{10.21469/22233792}
\receivedRus{25.02.2021}
\receivedEng{February 25, 2021}


\maketitle
\linenumbers

\section{Введение}

В последнее время растет интерес к разработке алгоритмических решений для автоматизации процесса проектирования архитектуры. Лучшие существующие алгоритмы поиска архитектуры требуют больших вычислительных затрат, несмотря на их высокое качество.

В данной работе рассматривается задача поиска архитектуры модели глубокого обучения с контролем её сложности. В качестве базового алгоритма используется дифференцируемый алгоритм поиска архитектуры (DARTS) \cite{journals/corr/abs-1806-09055}. Данный метод решает задачу поиска архитектуры модели путем перевода пространства поиска из дискретного в непрерывное представление. В связи с этим появляется возможность использовать градиентные методы оптимизации, позволяющие использовать меньше вычислительных ресурсов, чем методы, работающие на дискретном множестве. Данный алгоритм универсален для работы как со сверточными, так и с рекуррентными нейронными сетями.

В работе \cite{journals/corr/abs-2002-05283} стабильность алгоритма DARTS была поставлена под сомнение. Одним из источников нестабильности является этап получения фактической дискретной архитектуры из архитектуры непрерывной смеси. На этом этапе часто наблюдается снижение качества модели. Это связано с тем, что алгоритм сходится в узкий регион, поэтому  небольшие возмущения архитектуры ведут к значительному понижению качества на валидационной выборке. 

В работе \cite{journals/corr/abs-1911-12126} было замечено, что операция $softmax$ обладает существенным недостатком. Оказывается, что пропуск соединений постепенно становится доминирующим в процессе оптимизации. Вес данного соединения увеличивается гораздо быстрее, чем у его конкурентов. В связи с этим предлагается использовать сигмоидную  функцию потерь. Таким образом, каждая операция может давать существенный вклад независимо от других. Предлагаются альтернативные подходы к решению задачи поиска архитектуры модели. В работе \cite{journals/corr/abs-2006-10355} формулируется задача обучения распределению с ограничениями. Веса смешанной операции подчинены распределению Дирихле, так как они определены на вероятностном симплексе. Работе \cite{journals/corr/HaDL16} исследует гиперсети. Подход заключается в использовании небольшой сети для генерации весов более крупной сети. Рассматривались два варианта использования гиперсетей: статические гиперсети для генерации весов для сверточной сети и динамические гиперсети для генерации весов рекуррентной сети.

В работе \cite{journals/corr/abs-1912-12814} строится алгоритм поиска нейронной архитектуры с ограниченным ресурсом (RC-DARTS). К базовому алгоритму DARTS добавляются ресурсные ограничения, такие как размер модели и вычислительная сложность. Для решения задачи условной оптимизации вводится алгоритм итерационной проекции, поскольку функции ограничений невыпуклые.


Вычислительный эксперимент проводится на выборках MNIST\cite{lecun-mnisthandwrittendigit-2010} и CIFAR-10\cite{cif}.

\section{Постановка задачи}

\subsection{Дифференцируемый алгоритм поиска архитектуры}

 Пусть внутри ячейки есть $N$ узлов, пердставленных в виде ориентированного ациклического графа. Каждому ребру $(i, j)$ поставлена в соответствие операция $o^{(i, j)} \in \mathcal{O}$, где $\mathcal{O}$ -- множество операций. Для того, чтобы свести задачу дискретной оптимизации к задаче непрерывной оптимизации, введем смешанную операцию для каждого ребра $(i, j)$:

\begin{equation}
\hat{o}^{(i, j)}(x) = \sum_{o\in \mathcal{O}} \frac{\exp(\alpha_o^{(i, j)})}{\sum_{o'\in\mathcal{O}} \exp(\alpha_{o'}^{(i, j)})}o(x),
\end{equation}
где $\alpha_o^{(i, j)}$ обозначает соответствующий вес операции $o$ на ребре $(i, j)$. Таким образом, каждому ребру $(i, j)$ ставится в соответствие вертор $\alpha^{(i, j)}$ размерности $|\mathcal{O}|$. Пусть $\alpha = [\alpha^{(i, j)}]$. Сформулируем двухуровневую задачу оптимизации:

\begin{equation}
\begin{aligned}
\min_{\alpha}\mathcal{L}_{val}(\mathbf{w}^*(\alpha), \alpha),\\
 \mathrm{s.t.}\quad \mathbf{w}^* = \arg\min_{\mathbf{w}}\mathcal{L}_{train}(\mathbf{w}, \alpha)
 \end{aligned}
\end{equation}
Здесь $\mathcal{L}_{val}$ и $\mathcal{L}_{train}$ функции потерь модели на валидации и на обучении соответственно.


\begin{algorithm}[H]
\begin{algorithmic}[1]
\caption{DARTS -- Differentiable Architecture Search}
\STATE Для каждого узла создадим смешанную операцию $\hat{o}^{(i, j)}$, параметризованную $\alpha^{(i, j)}$
\WHILE{алгоритм не сошелся} 
\STATE  обновим $\alpha$, сделав градиентный шаг вдоль $\nabla_\alpha \mathcal{L}_{val}(\mathbf{w} - \xi\nabla_{\mathbf{w}}\mathcal{L}_{train}(\mathbf{w}, \alpha), \alpha)$
\STATE обновим веса $\mathbf{w}$, делая градиентный шаг вдоль $\nabla_\mathbf{w}\mathcal{L}_{train}(\mathbf{w}, \alpha)$
\ENDWHILE
\STATE получить окончательную архитектуру из полученного в результате алгоритма $\alpha$
\end{algorithmic}
\end{algorithm}
\subsection{Линейная гиперсеть}

Пусть $\Lambda$ --  множество параметров, контролирующие сложность модели. Под гиперсетью мы будем понимать следующее отображение:

\begin{equation}
	\mathbf{G} : \Lambda \times \mathbb{U} \rightarrow \mathbb{W},
\end{equation}

где $\mathbb{W}$ -- множество параметров модели, а $\mathbb{U}$ -- множество параметров гиперсети.

 В данной работе рассматривается линейная гиперсеть:
 
 \begin{equation}
 \mathbf{G}_{linear}(\lambda) = \lambda \mathbf{b}_1 + \mathbf{b}_2,
 \end{equation}
 где $\mathbf{b}_1, \mathbf{b}_2$ -- не зависящие от $\lambda$ константы.

%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/
\bibliography{Version1.bib}
\end{document}

