\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\bibliographystyle{jmlda-rus.bst}
\newcommand{\hdir}{.}

\usepackage{graphicx}
%\graphicspath{ {../fig/} {.}}
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{multirow}



\begin{document}

\title
    [Дифференцируемый алгоритм поиска архитектуры модели с контролем её сложности] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Дифференцируемый алгоритм поиска архитектуры модели с контролем её сложности}
\author
    [К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев} % основной список авторов, выводимый в оглавление
    [К.\,Д.~Яковлев, О.\,С.~Гребенькова, О.\,Ю.~Бахтеев] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    { iakovlev.kd@phystech.edu; grebenkova.os@phystech.edu; bakhteev@phystech.edu}

\abstract
    {В работе исследуется задача построения модели глубокого обучения. Предлагается метод поиска архитектуры модели, позволяющий контролировать её сложность с небольшими вычислительными затратами. Под сложностью модели понимается минимальная длина описания,
минимальное количество информации, которое требуется для передачи информации о модели и выборке. В основе метода лежит дифференцируемый алгоритм поиска архитектуры модели (DARTS). Предлагается использовать гиперсеть в качестве функции релаксации.  Под гиперсетью понимается модель, генерирующуя параметры другой модели. Предложенный метод позволяет контролировать сложность модели в процессе поиска архитектуры. Для оценки качества предлагаемого алгоритма проводятся эксперименты на выборке MNIST.
	
\bigskip
\noindent
\textbf{Ключевые слова}: \emph {дифференцируемый алгоритм поиска архитектуры; глубокое обучение; гиперсети; нейронные сети; контроль сложности модели}
}



%данные поля заполняются редакцией журнала
%\doi{10.21469/22233792}
\receivedRus{25.02.2021}
\receivedEng{February 25, 2021}


\maketitle
\linenumbers

\section{Введение}


В данной работе рассматривается задача поиска архитектуры модели глубокого обучения с контролем её сложности. Под моделью понимается суперпозиция функций, решающая задачу классификации или регрессии \cite{journals/aarc/BakhteevS18}. Под поиском архитектуры модели понимается поиск архитектуры ячейки. В качестве базового алгоритма используется дифференцируемый алгоритм поиска архитектуры (DARTS) \cite{journals/corr/abs-1806-09055}. Данный метод решает задачу поиска архитектуры модели путем перевода пространства поиска из дискретного в непрерывное представление. В связи с этим появляется возможность использовать градиентные методы оптимизации, позволяющие использовать меньше вычислительных ресурсов, чем методы, работающие на дискретном множестве. Данный алгоритм работает как со сверточными, так и с рекуррентными нейронными сетями.

В работе \cite{journals/corr/abs-2002-05283} было указано, что DARTS нестабилен. Одним из источников нестабильности является этап получения фактической дискретной архитектуры из архитектуры непрерывной смеси. На этом этапе наблюдается снижение качества модели. Это связано с тем, что параметры архитектуры модели сходятся в узкий регион, поэтому  небольшие возмущения архитектуры ведут к значительному понижению качества на валидационной выборке. 

В работе \cite{journals/corr/abs-1911-12126} было замечено, что операция $\text{softmax}$ обладает существенным недостатком. Некоторые компоненты вектора параметров архитектуры увеличиваются быстрее, чем остальные. В связи с этим предлагается использовать сигмоидную  функцию потерь и отказаться от нормировки. Таким образом, все компоненты изменяются с одинаковой скоростью. В данной работе предлагается в качестве функции релаксации использовать гиперсеть \cite{journals/corr/HaDL16}. Подход заключается в использовании небольшой сети для генерации параметров архитектуры другой сети.
 Предлагаются альтернативные подходы к решению задачи поиска архитектуры модели. В работе \cite{journals/corr/abs-2006-10355} формулируется задача обучения распределению. Параметры архитектуры подчинены распределению Дирихле, так как они определены на вероятностном симплексе. Таким образом, задача поиска архитектуры сводится к поиску параметров распределения Дирихле.

В работе \cite{journals/corr/abs-1912-12814} строится метод поиска нейронной архитектуры с ограниченным ресурсом (RC-DARTS). К базовому алгоритму DARTS добавляются ограничения, такие как число параметров модели. Для решения задачи условной оптимизации вводится алгоритм итерационной проекции, заключающийся в том, что через определенное число итераций градиентного спуска происходит проецирование на множество, задаваемое ограничениями. В данной работе для контроля сложности используется гиперсеть.


Вычислительный эксперимент проводится на выборке MNIST\cite{lecun-mnisthandwrittendigit-2010}.

\section{Постановка задачи}

\subsection{Дифференцируемый алгоритм поиска архитектуры ячейки}

Поставим задачу поиска архитектуры ячейки.
 Ячейка представляет собой $N$ занумерованных узлов, пердставленных в виде ориентированного ациклического графа. Каждому ребру $(i, j)$ поставлено в соответствие  отображение (операция) $o^{(i, j)} \in \mathcal{O}$, где $\mathcal{O}$ -- семейство отображений(множество операций). Значения в каждом из промежуточных узлов $x^{(j)}$ определяются через значения в узлах с меньшим номером:
 
 \begin{equation}
 x^{(j)} = \sum_{i < j}o^{(i, j)}(x^{(i)})
 \end{equation}
 
 Таким образом, задача поиска архитектуры заключается в выборе операций между узлами ячейки. Для того, чтобы свести задачу дискретной оптимизации к задаче непрерывной оптимизации, введем смешанную операцию для каждого ребра $(i, j)$:

\begin{equation}
\hat{o}^{(i, j)}(x) = \sum_{o\in \mathcal{O}} \frac{\exp(\alpha_o^{(i, j)})}{\sum_{o'\in\mathcal{O}} \exp(\alpha_{o'}^{(i, j)})}o(x),
\end{equation}
где $\alpha_o^{(i, j)}$ обозначает соответствующий вес операции $o$ на ребре $(i, j)$. Таким образом, каждому ребру $(i, j)$ ставится в соответствие вектор $\alpha^{(i, j)}$ размерности $|\mathcal{O}|$. Пусть $\alpha = [\alpha^{(i, j)}]$. Сформулируем двухуровневую задачу оптимизации:

\begin{equation}
\label{optim}
\begin{aligned}
\min_{\alpha}\mathcal{L}_\text{val}(\mathbf{w}^*(\alpha), \alpha),\\
 \mathrm{s.t.}\quad \mathbf{w}^* = \arg\min_{\mathbf{w}}\mathcal{L}_\text{train}(\mathbf{w}, \alpha)
 \end{aligned}
\end{equation}
Здесь $\mathcal{L}_\text{val}$ и $\mathcal{L}_\text{train}$ функции потерь модели на валидации и на обучении соответственно.

\subsection{Линейная гиперсеть}

Пусть $\Lambda$ --  множество параметров, контролирующие сложность модели. Гиперсеть -- это следующее отображение:

\begin{equation}
	\mathbf{G} : \Lambda \times \mathbb{U} \rightarrow \mathbb{A},
\end{equation}
где $\mathbb{A}$ -- пространство параметров архитектуры $\alpha^{(i, j)}_o$, а $\mathbb{U}$ -- множество параметров гиперсети.
В данной работе для получения весов операций используется линейная гиперсеть:
 
 \begin{equation}\label{hypernet}
 \mathbf{G}_{\text{linear}}(\lambda) = \lambda \mathbf{b}_1 + \mathbf{b}_2,
 \end{equation}
 где $\mathbf{b}_1, \mathbf{b}_2$ настраиваются согласно оптимизационной задаче \ref{optim}.

\section{Описание алгоритма}
 В качестве базового алгоритма используется алгоритм, описанный в работе \cite{journals/corr/abs-1806-09055}. Идея состоит в приближении истинного градиента:
$$\nabla_\alpha\mathcal{L}_\text{val}(\mathbf{w}^*(\alpha), \alpha) \approx \nabla_\alpha\mathcal{L}_\text{val}(\mathbf{w} - \xi\nabla_\mathbf{w}\mathcal{L}_\text{train}(\mathbf{w}, \alpha), \alpha)$$
Пусть $\mathbf{w}'(\alpha) = \mathbf{w} - \xi\nabla_\mathbf{w}\mathcal{L}_\text{train}(\mathbf{w}, \alpha)$. Тогда получаем:
\begin{equation}
\label{darts:grad}
\nabla_\alpha\mathcal{L}_\text{val}(\mathbf{w}'(\alpha), \alpha) = \nabla_\alpha\mathcal{L}_\text{val}(\mathbf{w}, \alpha)\big|_{\mathbf{w} = \mathbf{w}'(\alpha)} - \xi\nabla^2_{\alpha, \mathbf{w}}\mathcal{L}_\text{train}(\mathbf{w}, \alpha)\nabla_\mathbf{w'}\mathcal{L}_\text{val}(\mathbf{w}'(\alpha), \alpha)
\end{equation}

Обозначим $\mathbf{w}^{\pm} = \mathbf{w}\pm\epsilon\nabla_\mathbf{w'}\mathcal{L}_\text{val}(\mathbf{w}'(\alpha), \alpha)$, где $\epsilon = 0.01/\|\nabla_\mathbf{w'}\mathcal{L}_\text{val}(\mathbf{w}'(\alpha), \alpha) \|_2$.
Тогда
\begin{equation}
\label{darts:hess_prod}
\nabla^2_{\alpha, \mathbf{w}}\mathcal{L}_\text{train}(\mathbf{w}, \alpha)\nabla_\mathbf{w'}\mathcal{L}_\text{val}(\mathbf{w}'(\alpha), \alpha) \approx \frac{\nabla_\alpha\mathcal{L}_\text{train}(\mathbf{w}^+, \alpha) - \nabla_\alpha\mathcal{L}_\text{train}(\mathbf{w}^-, \alpha)}{2\epsilon}
\end{equation}
Следовательно, градиент \ref{darts:grad} вычисляется за $O(\dim\mathbf{w} + \dim\alpha)$. Заметим, что без оценки \ref{darts:hess_prod} приходилось бы осуществлять $O(\dim\mathbf{w}\dim\alpha)$ операций. Приведем псевдокод алгоритма:

 \begin{algorithm}[H]
\begin{algorithmic}[1]
\caption{DARTS -- Differentiable Architecture Search}
\label{alg:darts}
\STATE Для каждого узла создадим смешанную операцию $\hat{o}^{(i, j)}$, параметризованную $\alpha^{(i, j)}$
\WHILE{алгоритм не сошелся} 
\STATE  обновить $\alpha$, сделав градиентный шаг вдоль $\nabla_\alpha \mathcal{L}_\text{val}\bigl(\mathbf{w} - \xi\nabla_{\mathbf{w}}\mathcal{L}_\text{train}(\mathbf{w}, \alpha), \alpha\bigr)$
\STATE обновить веса $\mathbf{w}$, сделав градиентный шаг вдоль $\nabla_\mathbf{w}\mathcal{L}_\text{train}(\mathbf{w}, \alpha)$
\ENDWHILE
\STATE получить окончательную архитектуру из полученного $\alpha$
\end{algorithmic}
\end{algorithm}
Дискретная архитектура получается следующим образом:

$$o^{(i, j)} =\arg\max_{o\in\mathcal{O}}\alpha_o^{(i, j)}$$

В предлагаемом алгоритме смешанная операция определяется как:
 
 $$\hat{o}^{(i, j)} = \sum_{o\in \mathcal{O}}\alpha^{(i, j)}_oo(x)$$
Вектор параметров архитектуры модели определяется линейной гиперсетью:

$$\alpha = \lambda\mathbf{b}_1 + \mathbf{b}_2$$
Одним из свойств предлагаемого решения является то, что изменение сложности модели происходит заменой параметра $\lambda$ гиперсети без дополнительного обучения.


 
 \section{Вычислительный эксперимент}
 
 \subsection{Базовый эксперимент}
 Целью базового эксперимента является получение зависимости качества работы алгоритма DARTS с регуляризатором от количества эпох при разных значениях параметра регуляризации.
 
 Предлагается использовать алгоритм DARTS с регуляризатором $\lambda\sum_{i=1}^{\dim\alpha}\mathbf{v}_i|\alpha_i|$ в качестве второго слагаемого в функции потерь, где $\mathbf{v}$ -- вектор весов опреаций. Чем сложнее операция, тем больше соответствующий вес. Вычислительный эксперимент проводится на выборке MNIST\cite{lecun-mnisthandwrittendigit-2010}, которая представляет собой набор рукописных цифр.
 
Эксперимент запускался 5 раз при значениях $\lambda \in \{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\}$. На рисунке \ref{fig:basic_exp} представлен график зависимости точности (precision) модели от числа эпох.

График показывает, что при большом значении параметра регуляризации качество сильно падает. Это связано с тем, что в этом случае не выгодно брать операции с большим весом. Таким образом, модель становится переупрощенной.

 Также из графика видно, что при $\lambda \in \{10^{-4}, 10^{-3}\}$ качество модели значительно возрастает, что связано с тем, что в архитектуре появляются операции, имеющие большой вес.

\begin{figure}[H]
\centering
  \includegraphics[width=0.8\textwidth]{new_basic_exp.eps}
  \caption{Зависимость точности модели от числа прошедших эпох для разынх параметров регуляризации}
  \label{fig:basic_exp}
\end{figure}

\subsection{Основной эксперимент}

Целью основного эксперимента является получение зависимости качества работы предложенного метода от параметра гиперсети $\lambda \in \{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\}$. Эксперимент проводится на выборке MNIST \cite{lecun-mnisthandwrittendigit-2010}. Рассматривался поиск архитектуры сверночной нейронной сети с одним слоем и четырьмя узлами. Контроль сложности производился линейной гиперсетью \ref{hypernet}. Обучение проводилось на протяжении 100 эпох. В процессе обучения параметр $\lambda$ выбирался равномерно из отрезка $[10^{-5}, 10^{-1}]$. Как видно из графика \ref{fig:main_exp}, для $\lambda = 0.1$ качество модели заметно хуже, чем для других значениях $\lambda$. Это связано с тем, что штраф за сложность модели становится большим, поэтому происходит переупрощение модели. Также для каждой эпохи и для каждого $\lambda \in \{10^{-4}, 10^{-3}, 10^{-2}\}$ качество модели практически не меняется. Это означает, что качество модели заметно не ухудшается при уменьшении сложности.

\begin{figure}[H]
\centering
  \includegraphics[width=0.8\textwidth]{main_100_exp.eps}
  \caption{Зависимость качества модели от числа прошедших эпох для разных параметров $\lambda$ гиперсети.}
  \label{fig:main_exp}
\end{figure}

\subsection{Анализ ошибки}

\begin{table}[H]
\centering
	\begin{tabular}{ |c|c|c|c|c| }
	\hline
	 \multirow{2}{*}{Модель} & \multicolumn{4}{c|}{Precision, \%} \\ \cline{2-5}
	 		& эпоха 30 & эпоха 50 & эпоха 70 & эпоха 100\\
	 \hline
 	Hypernet, $\lambda = 0.1$ &96.3500 & 95.4800 & 94.1200 & 90.6333  \\ 
 	Hypernet, $\lambda = 0.01$ &95.8900 & 96.7167& 91.0633 & 95.3133 \\ 
 	Hypernet, $\lambda = 0.001$ &95.9133 & 96.6200 & 90.6400 & 95.6400 \\ 
 	Hypernet, $\lambda = 0.0001$ &95.9733 & 96.5367 & 90.4067 & 95.7533\\
 	\hline
 	DARTS, $\lambda = 0.1$&86.6000 &87.3967 & 89.3433 & 89.5067 \\
 	DARTS, $\lambda = 0.01$& 84.1333 & 90.6333 &91.9100 & 92.7333\\
 	DARTS, $\lambda = 0.001$&97.6533 & 97.5800 & 98.0833 & 97.9467 \\
 	DARTS, $\lambda = 0.0001$& \textbf{98.5800} &  \textbf{98.8867}&\textbf{98.9467}&\textbf{99.2400} \\
 	\hline
\end{tabular}
\caption{\label{tab:exp} Результаты базового и основного экспериментов. Приведены значения качества моделей на валидации.}
\end{table}


Из тыблицы \ref{tab:exp} видно, что для модели Hypernet свойственно уменьшение качества на валидации с ростом номера эпохи. Это связано с тем, что происходит выбор более простой архитектуры, а именно, становится больше пропусков соединений. Данное свойство DARTS исследовалось в работе \cite{journals/corr/abs-1911-12126}.

\begin{figure}[H]
\centering
  \includegraphics[width=\textwidth]{learn_curve.eps}
  \caption{Зависимость качества на обучающей и тестовой выборках от числа прошедших эпох для разных значений параметров $\lambda$ регуляризатора в базовом эксперименте.}
  \label{fig:learn_curve}
\end{figure}
 Из графика \ref{fig:learn_curve} видно, что уже после 20 эпохикачество на обучающей и на тестовой выборках пререстаёт сильно меняться. Кроме того, данный график показывает отсутствие переобучения для всех $\lambda$.

\section{Заключение}

В данной работе рассматривалась задача поиска архитектуры модели с контролем сложности. Предложен метод, позволяющий контролировать сложность модели в процессе поиска архитектуры. Метод обладает тем свойством, что изменение сложности итоговой модели происходит заменой параметра $\lambda$ гиперсети без дополнительного обучения. Также результаты показывают, что данный метод сопоставим по качеству на валидационной выборке с DARTS.




%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/
\newpage
\bibliography{Version1.bib}
 \nocite{*}
\end{document}

